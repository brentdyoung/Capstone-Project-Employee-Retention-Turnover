# -*- coding: utf-8 -*-
"""Final Output Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oCObwdSCzzM9N09rl_Yh4u2FS-B0v9jL

These models only use the selected features from the feature selection analysis. This notebook does not include the Cluster analysis or SMOTE.

# Notebook and Data Setup
"""

# installing package that lets me use CSV files in google drive

# comment the line out below because it only needs to be run once
!pip install -U -q PyDrive

# ==================================
# Packages 
# ==================================

# data manipulation and visualization packages
import pandas as pd
import numpy as np
import random as rn
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline 

# modeling packages
from xgboost import XGBClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# model metrics and scoring
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import log_loss
from sklearn.metrics import classification_report

# packages specific to neural network
from keras import backend as K
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop
from keras.utils import to_categorical
import tensorflow as tf

# These are used for getting data from Google Drive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate using the four packages above and create the PyDrive client (i.e. login to get data from Google Drive)
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# Using the two lines below to hide a warning message about visualization
import warnings
warnings.filterwarnings("ignore")

# ==================================
# Read/Pull in Data
# ==================================

# Copied the link to the IBM Data set below in the variable called 'link'
# AKA test
link_holdout = 'https://drive.google.com/open?id=1mGEmpon2Vt3vE0Q0EsIW73xD5f6HE3qj'
# AKA training
link_full_data = 'https://drive.google.com/open?id=1b1eBnwxQTnhOGW1a75ZHZ25Ku9UrQTVz'
#link_full = 'https://drive.google.com/open?id=1yiFBwrxClbq6tx1Z2qx68NobCPXvRcAt'
# We only need the portion of the link after ?id=
fluff, id = link_holdout.split('=')
print (id) # Verify that you have everything after '='
fluff, id2 = link_full_data.split('=')
print (id2)
#fluff, id3 = link_full.split('=')
#print (id3) # Verify that you have everything after '='

# The id above looks good, now pull the data into a dataframe using the id
downloaded = drive.CreateFile({'id':id})
downloaded.GetContentFile('WA_Fn-UseC_-HR-Employee-Attrition_test.csv')  
# AKA the test set
df_holdout = pd.read_csv('WA_Fn-UseC_-HR-Employee-Attrition_test.csv')
# Dataset is now stored in a Pandas Dataframe

downloaded = drive.CreateFile({'id':id2})
downloaded.GetContentFile('WA_Fn-UseC_-HR-Employee-Attrition_train.csv')  
# AKA the training set
df_full_data = pd.read_csv('WA_Fn-UseC_-HR-Employee-Attrition_train.csv')
# Dataset is now stored in a Pandas Dataframe

#downloaded = drive.CreateFile({'id':id3})
#downloaded.GetContentFile('WA_Fn-UseC_-HR-Employee-Attrition_test.csv')  
#df_full = pd.read_csv('WA_Fn-UseC_-HR-Employee-Attrition_test.csv')
# Dataset is now stored in a Pandas Dataframe

# list all columns
list(df_full_data)

# Set EmployeeNumber as index
df_holdout.set_index('EmployeeNumber',inplace=True)
df_full_data.set_index('EmployeeNumber',inplace=True)
#df_full.set_index('EmployeeNumber',inplace=True)

# Drop columns irrelevant to our analysis
# drop_columns = ['EmployeeCount','EmployeeNumber','Over18','StandardHours']
drop_columns = ['BusinessTravel','Department','Education','EducationField','EmployeeCount','Gender','HourlyRate','JobLevel','JobRole','MonthlyIncome','MonthlyRate','Over18','PercentSalaryHike','PerformanceRating','StandardHours','StockOptionLevel','YearsInCurrentRole','YearsSinceLastPromotion','YearsWithCurrManager']

# AKA the test set
df_holdout = df_holdout.drop(drop_columns, axis=1)
# AKA the train set
df_full_data = df_full_data.drop(drop_columns, axis=1)
#df_full = df_full.drop(drop_columns, axis=1)

# Convert Attrition to binary format for models

# AKA the test set
df_holdout['Attrition']=df_holdout['Attrition'].apply(lambda x : 1 if x=='Yes' else 0)
# AKA the training set
df_full_data['Attrition']=df_full_data['Attrition'].apply(lambda x : 1 if x=='Yes' else 0)
#df_full['Attrition']=df_full['Attrition'].apply(lambda x : 1 if x=='Yes' else 0)

# Convert all other categorical fields into numerical fields
# AKA the test set
df_holdout=pd.get_dummies(df_holdout)
# AKA the training set
df_full_data=pd.get_dummies(df_full_data)
#df_full=pd.get_dummies(df_full)

# AKA the test set
df_holdout.head(1)

# AKA the train set
df_full_data.head(3)

"""# Split and Normalize Data"""

# Separating the features(x) from the attrition values (y) for shared test/train set
#X_test = df_test.drop(['Attrition'], axis=1)
#y_test = df_test['Attrition'].as_matrix()
#X_train = df_train.drop(['Attrition'], axis=1)
#y_train = df_train['Attrition'].as_matrix()



# Separating the features(x) from the attrition values (y) for full data set (AKA train set)
X = df_full_data.drop(['Attrition'], axis=1)
y = df_full_data['Attrition'].as_matrix()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

# Normalize the training data
sc = StandardScaler()
# Fit the scaler to the training data
X_train = sc.fit_transform(X_train)
# Then standardise both training and test sets with that scaler
X_test = sc.transform (X_test)

"""# Create Definitions for Model Training"""

# Function to Train and Test Machine Learning Model
def train_test_ml_model(X_train, y_train, X_test, Model):
    model.fit(X_train, y_train) # Train the Model
    y_pred = model.predict(X_test) # Use the Model for prediction

    # Test the Model
    cm = confusion_matrix(y_test, y_pred)
    #predictions = [round(value) for value in y_pred]
    accuracy = accuracy_score(y_test, y_pred)

    # Plot/Display the results
    # First, the confusion matrix
    cm_plot(cm, Model)
    print('Accuracy of the Model', Model, str(accuracy) + '%')
    # Add a space for readability
    print(' ')
    # Second, the roc curve plot
    roc_plot(X_test, y_test, model)
    # Third, Log Loss result
    print(' ')
    print('Log Loss: ', log_loss(y_test, y_pred))
        # Add a space for readability
    print(' ')
    print(' ')
    print(classification_report(y_test, y_pred))

# Function to plot confusion matrix
def cm_plot(cm, Model):
    plt.clf()
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)
    classNames = ['Negative', 'Positive']
    plt.title('Comparison of Prediction Result for ' + Model)
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    tick_marks = np.arange(len(classNames))
    plt.xticks(tick_marks, classNames, rotation=45)
    plt.yticks(tick_marks, classNames)
    s = [['TN','FP'], ['FN', 'TP']]
    for i in range(2):
        for j in range(2):
            plt.text(j,i, str(s[i][j])+" = "+str(cm[i][j]))
    plt.show()

#Function to plot ROC Curve

def roc_plot(X_test, y_test, model):
# calculate the fpr and tpr for all thresholds of the classification
  probs = model.predict_proba(X_test)
  preds = probs[:,1]
  fpr, tpr, threshold = roc_curve(y_test, preds)
  roc_auc = auc(fpr, tpr)


  plt.title('Receiver Operating Characteristic')
  plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
  plt.legend(loc = 'lower right')
  plt.plot([0, 1], [0, 1],'r--')
  plt.xlim([0, 1])
  plt.ylim([0, 1])
  plt.ylabel('True Positive Rate')
  plt.xlabel('False Positive Rate')
  plt.show()

# Function to display Cross-Validation Score

def cross_val(X, Y, Model):
  # Adding space for readability between functions
  print(' ')
  print(' ')
  from sklearn.model_selection import cross_val_score
  #train model with cv of 10
  cv_scores = cross_val_score(model, X, Y, cv=10)
  
  #Display the results
  print('List of Cross-Validation Scores:', cv_scores)
  print('Mean of Cross-Validation Scores:{}'.format(np.mean(cv_scores)))

"""# Model 1: XGBoost Classification
(Parallel Tree Gradient Boosting)
"""

Model = "XGBClassifier()" # Adds to title in viz
model = XGBClassifier() # Create the Model

train_test_ml_model(X_train, y_train, X_test, Model)
cross_val(X, y, Model)

"""# Model 2: K-Nearest Neighbors Classification (KNN)"""

# Attempt 1: Out of Box

#n_neighbors=5 out of the box
Model = "KNeighborsClassifier"
model = KNeighborsClassifier()

train_test_ml_model(X_train,y_train,X_test,Model)
cross_val(X, y, Model)

# Attempt 2: Configuration

Model = "KNeighborsClassifier"
model = KNeighborsClassifier(n_neighbors=8)

train_test_ml_model(X_train,y_train,X_test,Model)
cross_val(X, y, Model)

"""# Model 3: Random Forest Classification"""

# Attempt 1: Out of Box

# this uses default RF values
# Out of the box, RF will use Bootstrapping to resample data
Model = "RandomForestClassifier"
model = RandomForestClassifier()

train_test_ml_model(X_train,y_train,X_test,Model)
cross_val(X, y, Model)

# Attempt 2: Configuration
# n_estimators = number of trees in the forest
# max_features = the maximum number of features Random Forest is allowed to try in individual tree
# min_samples_leaf = leaf is the end node of a decision tree, a smaller leaf makes the model more prone to capturing noise in train data 

# commenting this model out cuz it takes awhile to run
#Model = "RandomForestClassifier"
#model = RandomForestClassifier(n_estimators=5000, random_state=0, max_features=.1, max_depth=15)
#train_test_ml_model(X_train,y_train,X_test,Model)
#cross_val(X, y, Model)

"""# Method 4: SVM"""

Model = "SVC"
model = SVC(probability=True) 
# Note, if we move forward with this model, we cannot use 'True'
# to product the ROC curve because the formatting remove the probability % and 
# results only in answers of 1 or 0.

train_test_ml_model(X_train,y_train,X_test,Model)
cross_val(X, y, Model)

"""# Method 4: LDA"""

Model = "LDA"
model = LinearDiscriminantAnalysis() 

train_test_ml_model(X_train,y_train,X_test,Model)
cross_val(X, y, Model)

"""# Method 5: Logisitic Regression"""

Model = "LogisticRegression"
model = LogisticRegression() 

train_test_ml_model(X_train,y_train,X_test,Model)
cross_val(X, y, Model)

# Transform holdout set to feed into model to get output
X_holdout = df_holdout.drop(['Attrition'], axis=1)
y_holdout = df_holdout['Attrition'].as_matrix()
# Apply transformation from earlier
X_holdout = sc.transform(X_holdout)

# Run the model on our full data so we can get the predictions in one dataset.
final = model.predict(X_holdout)
calc_prob = model.predict_proba(X_holdout)[:, 1]
Final_Output = df_holdout
Final_Output['Probability_of_Attrition'] = calc_prob
Final_Output['Final_Prediction'] = final
Final_Output.head()

# Save Dataframe to CSV
Final_Output.to_csv("Final_Output.csv")

# Exploration of Log Reg Coefficients
# Reference used:
# https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8

import statsmodels.api as sm
import statsmodels.formula.api as smf
logit_model=smf.Logit(y_train, X_train)
results=logit_model.fit()
print(results.summary2())

# Column names for reference above
df_column_name = pd.DataFrame(list(df_full_data.drop(['Attrition'], axis=1).columns.values))
df_column_name.index = np.arange(1, len(df_column_name) + 1)
df_column_name

import statsmodels.api as sm
import statsmodels.formula.api as smf
#logit_model=smf.Logit(y_train, X_train)
#results=logit_model.fit()
#print(results.summary2())

model= smf.logit(formula="Attrition~ Age + DailyRate + DistanceFromHome + EnvironmentSatisfaction + JobInvolvement + JobSatisfaction + NumCompaniesWorked + RelationshipSatisfaction + TotalWorkingYears + TrainingTimesLastYear + WorkLifeBalance + YearsAtCompany + MaritalStatus_Divorced + MaritalStatus_Married + MaritalStatus_Single + OverTime_No + OverTime_Yes", data= df_full_data).fit()
model.summary()

# GETTING THE ODDS RATIOS, Z-VALUE, AND 95% CI
model_odds = pd.DataFrame(np.exp(model.params), columns= ['OR'])
model_odds['z-value']= model.pvalues
model_odds[['2.5%', '97.5%']] = np.exp(model.conf_int())
model_odds

"""# Model 6: Neural Network"""

# Random seeds
np.random.seed(123)
rn.seed(123)
tf.set_random_seed(123)

# Convert Attrition to one-hot encoding for NN to be able to read
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

# Neural Network Architecture

# Create initial set of linear layers
model=Sequential()
# Now, add to our linear layers and note their neurons in each added layer
# Input dimension only needs to be noted for the first layer and it is the number of features/columns
model.add(Dense(input_dim=17, units=8, activation='relu', name='output_1'))
model.add(Dense(units=16, activation='relu', name='output_2'))
# Make sure output later has two neurons for each type of classification of attrition
model.add(Dense(units=2, activation='sigmoid'))

# Compile the Network
# More information on optimizer types:
# https://keras.io/optimizers/
model.compile(optimizer=Adam(lr=0.01), loss='binary_crossentropy', metrics=['accuracy'])
# loss='binary_crossentropy' specifies that your model should optimize the log 
# loss for binary classification.  
# metrics=['accuracy'] specifies that accuracy should be printed out

# Review NN configuration
model.summary()

History = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=1)

model.predict_classes(X_test)

#model.predict(X_test)

# Log Loss over time
plt.plot(History.history['loss'])
plt.plot(History.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['train', 'test'])
plt.show()

# Model accuracy over time
plt.plot(History.history['acc'])
plt.plot(History.history['val_acc'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.legend(['train', 'test'])
plt.show()

print(model.metrics_names)

model.evaluate(X_train,y_train)
# averaged loss and accuracy on train

model.evaluate(X_test,y_test)
# averaged loss and accuracy on text

